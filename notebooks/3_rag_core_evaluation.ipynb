{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03_rag_core_evaluation.ipynb\n",
    "\n",
    "# 1. Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from datasets import Dataset # For Ragas/DeepEval\n",
    "from tqdm.notebook import tqdm # For progress bars in notebooks\n",
    "\n",
    "# Add the project root to the system path to import from src\n",
    "# This handles cases where the notebook is in a subfolder like 'notebooks/'\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Verify path for debugging\n",
    "print(f\"Project root added to path: {project_root}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Contents of src/: {os.listdir(os.path.join(project_root, 'src'))}\") # Corrected path for os.listdir\n",
    "\n",
    "from src.rag_pipeline import RAGPipeline\n",
    "\n",
    "# For Ragas evaluation\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision\n",
    ")\n",
    "# Note: For Ragas metrics, you'll need an LLM to evaluate.\n",
    "# It can be a small local model or even a HuggingFace API endpoint if you choose.\n",
    "\n",
    "# For DeepEval evaluation\n",
    "from deepeval import evaluate as deepeval_evaluate\n",
    "from deepeval.metrics import (\n",
    "    FaithfulnessMetric,\n",
    "    AnswerRelevancyMetric,\n",
    "    ContextRecallMetric,\n",
    "    # TEMPORARILY COMMENTED OUT DUE TO ImportError:\n",
    "    # ContextRelevancyMetric,\n",
    "    # BiasMetric,\n",
    "    # ToxicityMetric\n",
    ")\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# --- Configuration ---\n",
    "# Adjust paths as necessary based on your project structure\n",
    "DATA_PATH = '../data/processed/processed_data.parquet' # Or your CSV file\n",
    "VECTOR_STORE_PATH = 'vector_store/' # Relative to notebooks/\n",
    "\n",
    "# Initialize RAG Pipeline\n",
    "# You can change the model_name here if you prefer a different LLM\n",
    "rag_pipeline = RAGPipeline(\n",
    "    model_name=\"google/gemma-2b-it\", # Try \"microsoft/phi-2\" or \"distilbert-base-uncased\" if Gemma is too slow/large\n",
    "    embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    vector_store_path=VECTOR_STORE_PATH\n",
    ")\n",
    "\n",
    "# 2. Load Models and FAISS Index\n",
    "print(\"Loading embedding model...\")\n",
    "rag_pipeline.load_embedding_model()\n",
    "\n",
    "print(\"Loading FAISS index and metadata...\")\n",
    "try:\n",
    "    rag_pipeline.load_faiss_index()\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please ensure you've run Task 2 (02_embedding_chunking.ipynb) locally to create these files.\")\n",
    "    # You might want to halt execution here or add more robust error handling\n",
    "    raise\n",
    "\n",
    "print(\"Loading LLM (this will take time and use significant RAM)...\")\n",
    "rag_pipeline.load_llm()\n",
    "\n",
    "if rag_pipeline.llm is None:\n",
    "    print(\"Failed to load LLM. Cannot proceed with RAG chain or evaluation.\")\n",
    "    # You might want to stop here or try a smaller model.\n",
    "    raise ValueError(\"LLM failed to load.\")\n",
    "\n",
    "print(\"Setting up RAG chain...\")\n",
    "rag_pipeline.setup_rag_chain()\n",
    "\n",
    "# 3. Define Test Questions and Ground Truths\n",
    "# Replace with your actual questions and expected answers relevant to your credit trust data\n",
    "# It's good practice to have at least 10-20 pairs for evaluation.\n",
    "test_data = [\n",
    "    {\n",
    "        \"question\": \"What is a credit score and why is it important?\",\n",
    "        \"ground_truth\": \"A credit score is a numerical representation of your creditworthiness, used by lenders to assess risk. It's important because it influences your ability to get loans, credit cards, mortgages, and even apartment rentals.\",\n",
    "        \"contexts\": [] # Will be filled by RAG output\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How can I improve my credit score?\",\n",
    "        \"ground_truth\": \"Improving your credit score involves paying bills on time, keeping credit utilization low, avoiding new credit applications too frequently, and regularly checking your credit report for errors.\",\n",
    "        \"contexts\": []\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What factors affect my credit report?\",\n",
    "        \"ground_truth\": \"Factors affecting your credit report include payment history, amounts owed, length of credit history, new credit, and credit mix.\",\n",
    "        \"contexts\": []\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can I dispute errors on my credit report?\",\n",
    "        \"ground_truth\": \"Yes, you can dispute errors on your credit report by contacting the credit bureaus (Experian, Equifax, TransUnion) and providing evidence of the inaccuracy.\",\n",
    "        \"contexts\": []\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is credit utilization?\",\n",
    "        \"ground_truth\": \"Credit utilization is the amount of credit you're using compared to your total available credit. Keeping it low (ideally below 30%) is good for your credit score.\",\n",
    "        \"contexts\": []\n",
    "    },\n",
    "    # Add more relevant test questions based on your dataset\n",
    "]\n",
    "\n",
    "# 4. Generate RAG Responses and Collect Data for Evaluation\n",
    "print(\"\\nGenerating RAG responses for evaluation dataset...\")\n",
    "rag_responses = []\n",
    "for item in tqdm(test_data, desc=\"Generating responses\"):\n",
    "    question = item[\"question\"]\n",
    "    \n",
    "    # Retrieve documents and get the answer\n",
    "    answer, context_used_str, retrieved_docs_metadata = rag_pipeline.query(question)\n",
    "    \n",
    "    # Extract actual text content from retrieved_docs_metadata\n",
    "    retrieved_contexts_list = [doc['text_content'] for doc in retrieved_docs_metadata]\n",
    "\n",
    "    rag_responses.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"ground_truth\": item[\"ground_truth\"],\n",
    "        \"contexts\": retrieved_contexts_list # This is the list of strings (chunks) used as context\n",
    "    })\n",
    "\n",
    "# Convert to Pandas DataFrame and then Hugging Face Dataset for Ragas/DeepEval\n",
    "rag_df = pd.DataFrame(rag_responses)\n",
    "eval_dataset = Dataset.from_pandas(rag_df)\n",
    "\n",
    "print(\"\\nRAG responses generated and dataset prepared.\")\n",
    "print(rag_df.head())\n",
    "\n",
    "# 5. Ragas Evaluation\n",
    "\n",
    "print(\"\\n--- Starting Ragas Evaluation ---\")\n",
    "\n",
    "# Wrap your local LLM for Ragas compatibility\n",
    "from langchain_core.outputs import Generation, LLMResult\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "class RagasLocalLLM:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def generate(self, messages, **kwargs):\n",
    "        full_prompt = \"\"\n",
    "        for msg in messages:\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                full_prompt += f\"Human: {msg.content}\\n\"\n",
    "            elif isinstance(msg, AIMessage):\n",
    "                full_prompt += f\"AI: {msg.content}\\n\"\n",
    "            else:\n",
    "                full_prompt += f\"{msg.content}\\n\" # Fallback for other message types\n",
    "\n",
    "        # Invoke the underlying HuggingFacePipeline\n",
    "        response = self.llm.invoke(full_prompt)\n",
    "        \n",
    "        # Ragas expects a specific format: a list of Generation objects\n",
    "        return LLMResult(generations=[[Generation(text=response)]])\n",
    "\n",
    "ragas_llm_for_metrics = RagasLocalLLM(rag_pipeline.llm)\n",
    "ragas_embeddings_for_metrics = rag_pipeline.embeddings\n",
    "\n",
    "# Note: Evaluation can take a long time on CPU, especially with many test cases.\n",
    "print(\"Running Ragas evaluation... This may take a very long time on CPU.\")\n",
    "try:\n",
    "    result = evaluate(\n",
    "        eval_dataset,\n",
    "        metrics=[\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_recall,\n",
    "            context_precision\n",
    "        ],\n",
    "        llm=ragas_llm_for_metrics,\n",
    "        embeddings=ragas_embeddings_for_metrics,\n",
    "        show_progress=True # Show progress bar\n",
    "    )\n",
    "    ragas_metrics_df = result.to_pandas()\n",
    "    print(\"\\nRagas Evaluation Results:\")\n",
    "    print(ragas_metrics_df)\n",
    "    print(\"\\nOverall Ragas Scores:\")\n",
    "    print(ragas_metrics_df.mean(numeric_only=True)) # Calculate mean scores\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during Ragas evaluation: {e}\")\n",
    "    print(\"Ragas often requires specific LLM integrations. Check Ragas documentation or try a smaller subset of data.\")\n",
    "\n",
    "\n",
    "# 6. DeepEval Evaluation\n",
    "\n",
    "print(\"\\n--- Starting DeepEval Evaluation ---\")\n",
    "\n",
    "# Create DeepEval test cases from your RAG responses\n",
    "deepeval_test_cases = []\n",
    "for item in rag_responses:\n",
    "    context_list = item[\"contexts\"] if isinstance(item[\"contexts\"], list) else [item[\"contexts\"]]\n",
    "\n",
    "    deepeval_test_cases.append(\n",
    "        LLMTestCase(\n",
    "            input=item[\"question\"],\n",
    "            actual_output=item[\"answer\"],\n",
    "            expected_output=item[\"ground_truth\"],\n",
    "            retrieval_context=context_list\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Define DeepEval metrics\n",
    "try:\n",
    "    # IMPORTANT: Ensure that the metrics you *do* import above are the only ones used here.\n",
    "    deepeval_metrics = [\n",
    "        FaithfulnessMetric(threshold=0.7, model=rag_pipeline.model_name),\n",
    "        AnswerRelevancyMetric(threshold=0.7, model=rag_pipeline.model_name),\n",
    "        ContextRecallMetric(threshold=0.7, model=rag_pipeline.model_name),\n",
    "        # If ContextRelevancyMetric etc. are still needed, we'd need to find a compatible deepeval version or an alternative.\n",
    "    ]\n",
    "\n",
    "    print(\"Running DeepEval evaluation... This may also take a very long time on CPU.\")\n",
    "    deepeval_results = deepeval_evaluate(deepeval_test_cases, metrics=deepeval_metrics, show_progress=True)\n",
    "\n",
    "    print(\"\\nDeepEval Evaluation Results:\")\n",
    "    for result in deepeval_results:\n",
    "        print(f\"Question: {result.input}\")\n",
    "        print(f\"Answer: {result.actual_output[:100]}...\") # Truncate for cleaner output\n",
    "        print(f\"Expected: {result.expected_output[:100]}...\") # Truncate for cleaner output\n",
    "        print(\"Metrics:\")\n",
    "        for metric in result.metrics:\n",
    "            print(f\"  - {metric.metric_name}: {metric.score} (Reason: {metric.reason})\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during DeepEval evaluation: {e}\")\n",
    "    print(\"DeepEval often requires specific LLM configurations for metrics. If errors persist, consider:\")\n",
    "    print(\"1. Setting `os.environ[\\\"OPENAI_API_KEY\\\"]` and using an OpenAI model for DeepEval's internal LLM (`model=\\\"gpt-3.5-turbo\\\"`).\")\n",
    "    print(\"2. Setting up a local Ollama server and using `deepeval.llm_pipeline.OllamaLLM` for metrics.\")\n",
    "    print(\"3. Checking DeepEval's documentation for local LLM integration.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Task 3 (RAG Core & Evaluation) Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
